{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3cb912e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff78c364",
   "metadata": {},
   "source": [
    "#### 2.2.1 Replay Buffer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "671da4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A simple replay buffer to store experiences for off-policy learning.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Adds an experience tuple to the buffer.\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Samples a random batch of experiences from the buffer.\n",
    "        \"\"\"\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return (\n",
    "            np.array(state),\n",
    "            np.array(action),\n",
    "            np.array(reward),\n",
    "            np.array(next_state),\n",
    "            np.array(done)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the current size of the buffer.\n",
    "        \"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce0787c",
   "metadata": {},
   "source": [
    "#### 2.2.2 Network Definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d21b3351",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy network (Actor) for SAC. Outputs mean and log standard deviation for a Gaussian policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256, log_std_min=-20, log_std_max=2):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mu_head = nn.Linear(hidden_dim, action_dim)\n",
    "        self.log_std_head = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = self.mu_head(x)\n",
    "        log_std = self.log_std_head(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        return mu, log_std\n",
    "\n",
    "    def sample(self, state):\n",
    "        \"\"\"\n",
    "        Samples an action from the policy distribution and computes its log probability.\n",
    "        \"\"\"\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        normal = Normal(mu, std)\n",
    "        z = normal.rsample()\n",
    "        action = torch.tanh(z)\n",
    "\n",
    "        # Enforcing action bounds with tanh squashing\n",
    "        log_prob = normal.log_prob(z) - torch.log(1 - action.pow(2) + 1e-6)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        return action, log_prob\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"\n",
    "    Q-value network (Critic) for SAC. Takes state and action as input, outputs a Q-value.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q_value = self.fc3(x)\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bfbb010",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACAgent:\n",
    "    \"\"\"\n",
    "    Soft Actor-Critic (SAC) Agent implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim,\n",
    "                 hidden_dim=256,\n",
    "                 learning_rate=3e-4,\n",
    "                 gamma=0.99,\n",
    "                 tau=0.005,\n",
    "                 alpha=0.2, # Initial temperature parameter\n",
    "                 target_entropy=None):\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau # For soft update of target networks\n",
    "        self.alpha = alpha # Temperature parameter\n",
    "\n",
    "        # Actor Network\n",
    "        self.actor = Actor(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Critic Networks (Two Q-networks)\n",
    "        self.critic1 = Critic(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.critic2 = Critic(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=learning_rate)\n",
    "        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Target Critic Networks (Copies for stability)\n",
    "        self.target_critic1 = Critic(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_critic2 = Critic(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_critic1.load_state_dict(self.critic1.state_dict())\n",
    "        self.target_critic2.load_state_dict(self.critic2.state_dict())\n",
    "\n",
    "        # Automatic entropy tuning (alpha)\n",
    "        if target_entropy is None:\n",
    "            self.target_entropy = -float(action_dim)  \n",
    "        else:\n",
    "            self.target_entropy = target_entropy\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=learning_rate)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Samples an action from the current policy for interaction with the environment.\n",
    "        \"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action, _ = self.actor.sample(state)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "\n",
    "    def update(self, replay_buffer, batch_size):\n",
    "        \"\"\"\n",
    "        Performs a single training update step for the SAC agent.\n",
    "        \"\"\"\n",
    "        if len(replay_buffer) < batch_size:\n",
    "            return None, None, None \n",
    "\n",
    "        # Sample batch from replay buffer\n",
    "        state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        action = torch.FloatTensor(action).to(device)\n",
    "        reward = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        done = torch.FloatTensor(done).unsqueeze(1).to(device)\n",
    "\n",
    "        # --- Update Critic Networks (Q-functions) ---\n",
    "        with torch.no_grad():\n",
    "            next_action, log_prob_next_action = self.actor.sample(next_state)\n",
    "            target_q1 = self.target_critic1(next_state, next_action)\n",
    "            target_q2 = self.target_critic2(next_state, next_action)\n",
    "            min_target_q = torch.min(target_q1, target_q2) - self.alpha * log_prob_next_action\n",
    "            target_q_value = reward + (1 - done) * self.gamma * min_target_q\n",
    "\n",
    "        current_q1 = self.critic1(state, action)\n",
    "        current_q2 = self.critic2(state, action)\n",
    "\n",
    "        loss_critic1 = F.mse_loss(current_q1, target_q_value)\n",
    "        loss_critic2 = F.mse_loss(current_q2, target_q_value)\n",
    "\n",
    "        self.critic1_optimizer.zero_grad()\n",
    "        loss_critic1.backward()\n",
    "        self.critic1_optimizer.step()\n",
    "\n",
    "        self.critic2_optimizer.zero_grad()\n",
    "        loss_critic2.backward()\n",
    "        self.critic2_optimizer.step()\n",
    "\n",
    "        # --- Update Actor Network (Policy) ---\n",
    "        new_action, log_prob_new_action = self.actor.sample(state)\n",
    "        q1_new_action = self.critic1(state, new_action)\n",
    "        q2_new_action = self.critic2(state, new_action)\n",
    "        min_q_new_action = torch.min(q1_new_action, q2_new_action)\n",
    "\n",
    "        actor_loss = (self.alpha * log_prob_new_action - min_q_new_action).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # --- Update Alpha (Temperature) ---\n",
    "        alpha_loss = -(self.log_alpha * (log_prob_new_action + self.target_entropy).detach()).mean()\n",
    "\n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimizer.step()\n",
    "\n",
    "        self.alpha = self.log_alpha.exp().item()\n",
    "\n",
    "        # --- Soft Update of Target Critic Networks ---\n",
    "        for param, target_param in zip(self.critic1.parameters(), self.target_critic1.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        for param, target_param in zip(self.critic2.parameters(), self.target_critic2.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        return loss_critic1.item(), actor_loss.item(), self.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7c1fa11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: HalfCheetah-v5\n",
      "State Dimension: 17\n",
      "Action Dimension: 6\n",
      "Max Action Value: 1.0\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = \"HalfCheetah-v5\"\n",
    "MAX_EPISODES = 150 \n",
    "MAX_STEPS_PER_EPISODE = 1000 \n",
    "BATCH_SIZE = 256\n",
    "REPLAY_BUFFER_CAPACITY = 1_000_000\n",
    "LEARNING_RATE = 3e-4\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005 \n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make(ENV_NAME)\n",
    "observation, info = env.reset(seed=0) \n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = env.action_space.high[0] \n",
    "\n",
    "print(f\"Environment: {ENV_NAME}\")\n",
    "print(f\"State Dimension: {state_dim}\")\n",
    "print(f\"Action Dimension: {action_dim}\")\n",
    "print(f\"Max Action Value: {max_action}\")\n",
    "\n",
    "# Initialize replay buffer and agent\n",
    "replay_buffer = ReplayBuffer(REPLAY_BUFFER_CAPACITY)\n",
    "agent = SACAgent(state_dim, action_dim,\n",
    "                 hidden_dim=HIDDEN_DIM,\n",
    "                 learning_rate=LEARNING_RATE,\n",
    "                 gamma=GAMMA,\n",
    "                 tau=TAU)\n",
    "\n",
    "# Training metrics\n",
    "episode_rewards = []\n",
    "actor_losses = []\n",
    "critic_losses = []\n",
    "alphas = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec41ac8",
   "metadata": {},
   "source": [
    "#### 2.2.5 Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ed9cbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Episode 10/150   Reward: -120.60   Avg 10 Episodes Reward: -250.28   Buffer Size: 10000   Alpha: 0.2263\n",
      "Episode 20/150   Reward: -348.83   Avg 10 Episodes Reward: -235.80   Buffer Size: 20000   Alpha: 0.0154\n",
      "Episode 30/150   Reward: 154.06   Avg 10 Episodes Reward: 161.55   Buffer Size: 30000   Alpha: 0.0069\n",
      "Episode 40/150   Reward: 2587.04   Avg 10 Episodes Reward: 1576.68   Buffer Size: 40000   Alpha: 0.0224\n",
      "Episode 50/150   Reward: 3281.70   Avg 10 Episodes Reward: 2470.23   Buffer Size: 50000   Alpha: 0.0414\n",
      "Episode 60/150   Reward: 3825.55   Avg 10 Episodes Reward: 3144.17   Buffer Size: 60000   Alpha: 0.0571\n",
      "Episode 70/150   Reward: 3505.34   Avg 10 Episodes Reward: 3829.64   Buffer Size: 70000   Alpha: 0.0777\n",
      "Episode 80/150   Reward: 3266.98   Avg 10 Episodes Reward: 4215.67   Buffer Size: 80000   Alpha: 0.0990\n",
      "Episode 90/150   Reward: 5263.56   Avg 10 Episodes Reward: 4813.44   Buffer Size: 90000   Alpha: 0.1223\n",
      "Episode 100/150   Reward: 5264.23   Avg 10 Episodes Reward: 5063.31   Buffer Size: 100000   Alpha: 0.1450\n",
      "Episode 110/150   Reward: 4681.94   Avg 10 Episodes Reward: 4977.83   Buffer Size: 110000   Alpha: 0.1554\n",
      "Early stopping at episode 110 due to high average reward.\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "WARMUP_STEPS = 5000 \n",
    "total_steps = 0 \n",
    "\n",
    "for episode in range(1, MAX_EPISODES + 1):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    \n",
    "    critic_loss = 0.0 \n",
    "    actor_loss = 0.0\n",
    "    alpha_val = agent.alpha \n",
    "\n",
    "\n",
    "    while not done and steps < MAX_STEPS_PER_EPISODE:\n",
    "        if total_steps < WARMUP_STEPS:\n",
    "            action = env.action_space.sample() # Take random action for warmup\n",
    "        else:\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action * max_action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        if total_steps >= WARMUP_STEPS: # update after warmup\n",
    "            temp_critic_loss, temp_actor_loss, temp_alpha_val = agent.update(replay_buffer, BATCH_SIZE)\n",
    "\n",
    "            if temp_critic_loss is not None:\n",
    "                critic_loss = temp_critic_loss\n",
    "                actor_loss = temp_actor_loss\n",
    "                alpha_val = temp_alpha_val\n",
    "\n",
    "                critic_losses.append(critic_loss)\n",
    "                actor_losses.append(actor_loss)\n",
    "                alphas.append(alpha_val)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        steps += 1\n",
    "        total_steps += 1 \n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-10:])\n",
    "        print(f\"\"\"Episode {episode}/{MAX_EPISODES}   Reward: {episode_reward:.2f}   Avg 10 Episodes Reward: {avg_reward:.2f}   Buffer Size: {len(replay_buffer)}   Alpha: {agent.alpha:.4f}\"\"\")\n",
    "\n",
    "    # Check for convergence (e.g., if average reward is consistently high)\n",
    "    if np.mean(episode_rewards[-100:]) > 3000 and episode > 100:\n",
    "        print(f\"Early stopping at episode {episode} due to high average reward.\")\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "print(\"Training finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
